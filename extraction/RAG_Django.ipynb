{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thisiskj/rag-chatbot/blob/main/extraction/RAG_Django.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l59P-1q0d4s6"
      },
      "source": [
        "# Web Crawler\n",
        "\n",
        "Install the required dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "shSiZWw0kAJm"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb langchain-text-splitters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crawl the site"
      ],
      "metadata": {
        "id": "Tm9uB8GJ8X02"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UJGCIP_Gcu3p"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import Comment\n",
        "from urllib.parse import urlparse, urljoin\n",
        "import concurrent\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import queue\n",
        "import threading\n",
        "import time\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from google.colab import userdata\n",
        "\n",
        "discovered = set()\n",
        "visited = set()\n",
        "lock = threading.Lock() # Lock for accessing the above 2 variables\n",
        "\n",
        "# https://docs.djangoproject.com/en/5.1/\n",
        "DOMAIN = 'docs.djangoproject.com'\n",
        "PATH_PREFIX = '/en/5.1/'\n",
        "BASE_URL = f'https://{DOMAIN}{PATH_PREFIX}'\n",
        "\n",
        "# Time to sleep between HTTP requests. Applies per thread\n",
        "SLEEP_TIME = 0.25\n",
        "\n",
        "# Doc splitter\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False\n",
        ")\n",
        "\n",
        "# Chroma DB\n",
        "# chroma_client = chromadb.PersistentClient(path=\"/content/drive/MyDrive/Chroma_Django\")\n",
        "chroma_client = chromadb.HttpClient(\n",
        "    host='chroma-production-e1c6.up.railway.app',\n",
        "    port=443,\n",
        "    ssl=True,\n",
        "    settings=Settings(\n",
        "        chroma_client_auth_provider=\"chromadb.auth.token_authn.TokenAuthClientProvider\",\n",
        "        chroma_client_auth_credentials=userdata.get(\"CHROMA_CLIENT_AUTH_CREDENTIALS\"),\n",
        "        chroma_auth_token_transport_header=\"X-Chroma-Token\"\n",
        "    )\n",
        ")\n",
        "# chroma_client.delete_collection(name=\"django_docs\")\n",
        "collection = chroma_client.create_collection(\n",
        "    name=\"django_docs\",\n",
        "    get_or_create=True\n",
        ")\n",
        "\n",
        "\n",
        "# Given a BS4 object, extract the page content\n",
        "def extract_main_content(soup):\n",
        "    # Remove unwanted elements\n",
        "    for element in soup(['script', 'style', 'header', 'footer', 'nav', 'aside', 'noscript', 'iframe']):\n",
        "        element.extract()\n",
        "\n",
        "    # Function to filter visible text\n",
        "    def is_visible(element):\n",
        "        if isinstance(element, Comment):\n",
        "            return False\n",
        "        parent = element.parent.name\n",
        "        if parent in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    # Extract visible text elements\n",
        "    texts = soup.findAll(string=True)\n",
        "    visible_texts = filter(is_visible, texts)\n",
        "\n",
        "    # Join the texts and clean up whitespace\n",
        "    text = ' '.join(t.strip() for t in visible_texts if t.strip())\n",
        "\n",
        "    return text\n",
        "\n",
        "def extract_links(url):\n",
        "    headers = {\n",
        "      \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "    time.sleep(SLEEP_TIME)\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Get links on page\n",
        "    links = [link.get('href') for link in soup.find_all('a')]\n",
        "    discovered_links = filter_links(url, links)\n",
        "    with lock:\n",
        "        visited.add(url)\n",
        "        print(f\"[{threading.get_native_id()}] Found {len(discovered_links)} from URL {url} visited {len(visited)}/{len(discovered)} sites ({int(len(visited)/len(discovered)*100)}%). discovered_links: {discovered_links}\")\n",
        "\n",
        "    # Store content page content\n",
        "    page_content = extract_main_content(soup)\n",
        "    page_chunks = splitter.split_text(page_content)\n",
        "    collection.add(\n",
        "        documents=page_chunks,\n",
        "        ids=[f\"url|||{url}|||{i}\" for i, pc in enumerate(page_chunks)],\n",
        "        metadatas=[{\"source\": url} for pc in page_chunks],\n",
        "    )\n",
        "\n",
        "    return discovered_links\n",
        "\n",
        "def filter_links(source_url, links):\n",
        "    filtered_links = []\n",
        "    source = urlparse(source_url)\n",
        "    for link in links:\n",
        "        parsed = urlparse(link)\n",
        "\n",
        "        # print(source)\n",
        "        # print(parsed)\n",
        "\n",
        "        # Remove fragment\n",
        "        parsed = parsed._replace(fragment='')\n",
        "\n",
        "        # Set scheme, if empty\n",
        "        if parsed.scheme == '':\n",
        "            parsed = parsed._replace(scheme='https')\n",
        "\n",
        "        # Set netloc, if empty\n",
        "        if parsed.netloc == '':\n",
        "            parsed = parsed._replace(netloc=DOMAIN)\n",
        "\n",
        "        # If path is relative to current page, add prefix\n",
        "        if not parsed.path.startswith('/'):\n",
        "            join = '/' if source.path.endswith('') else ''\n",
        "            parsed = parsed._replace(path=f'{source.path}{join}{parsed.path}')\n",
        "\n",
        "        # print(parsed)\n",
        "        final_url = parsed.geturl()\n",
        "        # print(final_url)\n",
        "\n",
        "        if final_url.startswith('https://docs.djangoproject.com/en/5.1/releases'):\n",
        "            continue\n",
        "\n",
        "        if final_url.startswith(BASE_URL):\n",
        "            final_url = final_url.rstrip('/')\n",
        "            final_url = urljoin(final_url, final_url.split('/')[-1])\n",
        "            # print('adding to frontier:')\n",
        "            # print(final_url)\n",
        "            # Append to discovery list and return\n",
        "            # The returned filtered_urls will be crawled\n",
        "            with lock:\n",
        "                if final_url and final_url not in discovered:\n",
        "                    filtered_links.append(final_url)\n",
        "                    discovered.add(final_url)\n",
        "        # print(\"--\"*20)\n",
        "    return filtered_links\n",
        "\n",
        "# The q always holds the futures!\n",
        "q = []\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=32) as executor:\n",
        "    q.append(executor.submit(extract_links, BASE_URL))\n",
        "    while len(q) > 0:\n",
        "        for future in concurrent.futures.as_completed(q):\n",
        "            q.remove(future)\n",
        "            try:\n",
        "                links = future.result()\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                continue\n",
        "            for link in links:\n",
        "                q.append(executor.submit(extract_links, link))\n",
        "\n",
        "# print(extract_links('https://docs.djangoproject.com/en/5.1/topics/db/search'))\n",
        "print('DONE')\n",
        "print(discovered)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Qmo2skd2HWDrqzZqaKdiHvOlPtGUZV4i",
      "authorship_tag": "ABX9TyPxBPT0jWuiSeYmunTqxENS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}